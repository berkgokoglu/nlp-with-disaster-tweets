{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### https://curiousily.com/posts/sentiment-analysis-with-bert-and-hugging-face-using-pytorch-and-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip3 install -q -U watermark\n",
    "!pip install -qq transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = Path('/home/sharif/Documents/Challenges/nlp-with-disaster-tweets/data')\n",
    "train, test = pd.read_csv(DATA/'train.csv'), pd.read_csv(DATA/'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRE_TRAINED_MODEL_NAME = 'bert-base-cased'\n",
    "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_txt = 'When was I last outside? I am stuck at home for 2 weeks.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['When',\n",
       " 'was',\n",
       " 'I',\n",
       " 'last',\n",
       " 'outside',\n",
       " '?',\n",
       " 'I',\n",
       " 'am',\n",
       " 'stuck',\n",
       " 'at',\n",
       " 'home',\n",
       " 'for',\n",
       " '2',\n",
       " 'weeks',\n",
       " '.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(sample_txt); tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1332,\n",
       " 1108,\n",
       " 146,\n",
       " 1314,\n",
       " 1796,\n",
       " 136,\n",
       " 146,\n",
       " 1821,\n",
       " 5342,\n",
       " 1120,\n",
       " 1313,\n",
       " 1111,\n",
       " 123,\n",
       " 2277,\n",
       " 119]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids = tokenizer.convert_tokens_to_ids(tokens); token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding = tokenizer.encode_plus(\n",
    "    sample_txt,\n",
    "    max_length=32,\n",
    "    add_special_tokens=True,\n",
    "    return_token_type_ids=False,\n",
    "    pad_to_max_length=True,\n",
    "    return_attention_mask=True,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "encoding.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 101, 1332, 1108,  146, 1314, 1796,  136,  146, 1821, 5342, 1120, 1313,\n",
       "        1111,  123, 2277,  119,  102,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(encoding['input_ids'][0]))\n",
    "encoding['input_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(encoding['attention_mask'][0]))\n",
    "encoding['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'When',\n",
       " 'was',\n",
       " 'I',\n",
       " 'last',\n",
       " 'outside',\n",
       " '?',\n",
       " 'I',\n",
       " 'am',\n",
       " 'stuck',\n",
       " 'at',\n",
       " 'home',\n",
       " 'for',\n",
       " '2',\n",
       " 'weeks',\n",
       " '.',\n",
       " '[SEP]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(encoding['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "toke_lens = []\n",
    "for txt in train.text:\n",
    "    tokens = tokenizer.encode(txt, max_length=512)\n",
    "    toke_lens.append(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(toke_lens)\n",
    "plt.xlim([0, 256]);\n",
    "plt.xlabel('Token count');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DS(Dataset):\n",
    "    def __init__(self, tweets, targets, tokenizer, max_len):\n",
    "        self.tweets = tweets\n",
    "        self.targets = targets\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self): return len(self.tweets)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        tweet = str(self.tweets[item])\n",
    "        target = self.targets[item]\n",
    "        \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "          tweet,\n",
    "          add_special_tokens=True,\n",
    "          max_length=self.max_len,\n",
    "          return_token_type_ids=False,\n",
    "          pad_to_max_length=True,\n",
    "          return_attention_mask=True,\n",
    "          return_tensors='pt',\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "          'tweet_text': tweet,\n",
    "          'input_ids': encoding['input_ids'].flatten(),\n",
    "          'attention_mask': encoding['attention_mask'].flatten(),\n",
    "          'targets': torch.tensor(target, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_valid = train_test_split(train, test_size=0.3)\n",
    "df_valid, df_test = train_test_split(df_valid, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5329, 1142, 1142)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_train), len(df_valid), len(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
    "    ds = DS(\n",
    "        tweets=df.text.to_numpy(),\n",
    "        targets=df.target.to_numpy(),\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=max_len\n",
    "    )\n",
    "    \n",
    "    return DataLoader(\n",
    "        ds,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=4\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "val_data_loader = create_data_loader(df_valid, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "test_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = next(iter(train_data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['tweet_text', 'input_ids', 'attention_mask', 'targets'])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12, 120])\n",
      "torch.Size([12, 120])\n",
      "torch.Size([12])\n"
     ]
    }
   ],
   "source": [
    "print(data['input_ids'].shape)\n",
    "print(data['attention_mask'].shape)\n",
    "print(data['targets'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_hidden_state, pooled_output = bert_model(\n",
    "    input_ids=encoding['input_ids'],\n",
    "    attention_mask=encoding['attention_mask']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 768])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_model.config.hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pooled_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "        self.drop = nn.Dropout(p=0.3)\n",
    "        self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
    "            \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        _, pooled_output = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        output = self.drop(pooled_output)\n",
    "        return self.out(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "model = Classifier(2)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = data['input_ids'].cuda()\n",
    "attention_mask = data['attention_mask'].cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5375, 0.4625],\n",
       "        [0.6119, 0.3881],\n",
       "        [0.6358, 0.3642],\n",
       "        [0.3789, 0.6211],\n",
       "        [0.4605, 0.5395],\n",
       "        [0.4806, 0.5194],\n",
       "        [0.5753, 0.4247],\n",
       "        [0.3595, 0.6405],\n",
       "        [0.6341, 0.3659],\n",
       "        [0.5262, 0.4738],\n",
       "        [0.3965, 0.6035],\n",
       "        [0.5426, 0.4574]], device='cuda:0', grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(model(input_ids, attention_mask), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 1000\n",
    "\n",
    "optim = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n",
    "total_steps = len(train_data_loader) * EPOCHS\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "  optim,\n",
    "  num_warmup_steps=0,\n",
    "  num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, data_loader, loss_fn, optim, device, scheduler, n_examples):\n",
    "    model.train()\n",
    "    \n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    \n",
    "    for i, d in enumerate(data_loader):\n",
    "        #if i % 50 == 0: print(f'{i}/{len(data_loader)}')\n",
    "        input_ids = d[\"input_ids\"].to(device)\n",
    "        attention_mask = d[\"attention_mask\"].to(device)\n",
    "        targets = d[\"targets\"].to(device)\n",
    "        \n",
    "        outputs = model(\n",
    "          input_ids=input_ids,\n",
    "          attention_mask=attention_mask\n",
    "        )\n",
    "        \n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        \n",
    "        correct_predictions += torch.sum(preds == targets)\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optim.step()\n",
    "        scheduler.step()\n",
    "        optim.zero_grad()\n",
    "        \n",
    "    return correct_predictions.double() / n_examples, np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
    "    model = model.eval()\n",
    "\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, d in enumerate(data_loader):\n",
    "            #if i % 50 == 0: print(f'{i}/{len(data_loader)}')\n",
    "            input_ids = d[\"input_ids\"].to(device)\n",
    "            attention_mask = d[\"attention_mask\"].to(device)\n",
    "            targets = d[\"targets\"].to(device)\n",
    "\n",
    "            outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "            )\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "\n",
    "            loss = loss_fn(outputs, targets)\n",
    "\n",
    "            correct_predictions += torch.sum(preds == targets)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "    return correct_predictions.double() / n_examples, np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "----------\n",
      "Train loss 0.5546153318513645 accuracy 0.7402889848001502\n",
      "Val   loss 0.5093026825925335 accuracy 0.7959719789842382\n",
      "\n",
      "Epoch 2/1000\n",
      "----------\n",
      "Train loss 0.40833674669265746 accuracy 0.8399324451116532\n",
      "Val   loss 0.5422503683366813 accuracy 0.8161120840630472\n",
      "\n",
      "Epoch 3/1000\n",
      "----------\n",
      "Train loss 0.3154275940082381 accuracy 0.8984800150121974\n",
      "Val   loss 0.5582649462254873 accuracy 0.8353765323992994\n",
      "\n",
      "Epoch 4/1000\n",
      "----------\n",
      "Train loss 0.25015037978549354 accuracy 0.9277537999624695\n",
      "Val   loss 0.6543993910114901 accuracy 0.8345008756567426\n",
      "\n",
      "Epoch 5/1000\n",
      "----------\n",
      "Train loss 0.2018848820353055 accuracy 0.9472696565959843\n",
      "Val   loss 0.8667145679864916 accuracy 0.8213660245183888\n",
      "\n",
      "Epoch 6/1000\n",
      "----------\n",
      "Train loss 0.15682347718648498 accuracy 0.9615312441358604\n",
      "Val   loss 0.9621432783642376 accuracy 0.8222416812609458\n",
      "\n",
      "Epoch 7/1000\n",
      "----------\n",
      "Train loss 0.12036546579695548 accuracy 0.9720397823231376\n",
      "Val   loss 1.1894108307915303 accuracy 0.7950963222416813\n",
      "\n",
      "Epoch 8/1000\n",
      "----------\n",
      "Train loss 0.09820554067702504 accuracy 0.9757928316757366\n",
      "Val   loss 1.357549616677261 accuracy 0.8021015761821366\n",
      "\n",
      "Epoch 9/1000\n",
      "----------\n",
      "Train loss 0.09137433999331845 accuracy 0.9774817038844061\n",
      "Val   loss 1.1680543632476958 accuracy 0.8021015761821366\n",
      "\n",
      "Epoch 10/1000\n",
      "----------\n",
      "Train loss 0.07958810286101979 accuracy 0.981422405704635\n",
      "Val   loss 1.3932651405957586 accuracy 0.8038528896672504\n",
      "\n",
      "Epoch 11/1000\n",
      "----------\n",
      "Train loss 0.07233201807065281 accuracy 0.9836742353161945\n",
      "Val   loss 1.1991501961665563 accuracy 0.8064798598949212\n",
      "\n",
      "Epoch 12/1000\n",
      "----------\n",
      "Train loss 0.06336643453631838 accuracy 0.9842371927190843\n",
      "Val   loss 1.4378012801147027 accuracy 0.7898423817863398\n",
      "\n",
      "Epoch 13/1000\n",
      "----------\n",
      "Train loss 0.055164832812688164 accuracy 0.9868643272659036\n",
      "Val   loss 1.3190109682454931 accuracy 0.7863397548161121\n",
      "\n",
      "Epoch 14/1000\n",
      "----------\n",
      "Train loss 0.0517379319710589 accuracy 0.988928504409833\n",
      "Val   loss 1.4175158112648205 accuracy 0.7863397548161121\n",
      "\n",
      "Epoch 15/1000\n",
      "----------\n",
      "Train loss 0.04583002890406564 accuracy 0.9881778945393133\n",
      "Val   loss 1.556822959020489 accuracy 0.7915936952714536\n",
      "\n",
      "Epoch 16/1000\n",
      "----------\n",
      "Train loss 0.05079726793640097 accuracy 0.989116156877463\n",
      "Val   loss 1.3501860859729884 accuracy 0.8029772329246935\n",
      "\n",
      "Epoch 17/1000\n",
      "----------\n",
      "Train loss 0.047643500559615345 accuracy 0.9902420716832426\n",
      "Val   loss 1.503966624856427 accuracy 0.7653239929947461\n",
      "\n",
      "Epoch 18/1000\n",
      "----------\n",
      "Train loss 0.03988381737434067 accuracy 0.9902420716832426\n",
      "Val   loss 1.5491631385934852 accuracy 0.8047285464098074\n",
      "\n",
      "Epoch 19/1000\n",
      "----------\n",
      "Train loss 0.03700414069786307 accuracy 0.9911803340213924\n",
      "Val   loss 1.6606067568712508 accuracy 0.766199649737303\n",
      "\n",
      "Epoch 20/1000\n",
      "----------\n",
      "Train loss 0.032523074263564225 accuracy 0.9917432914242823\n",
      "Val   loss 1.517511361284657 accuracy 0.8239929947460596\n",
      "\n",
      "Epoch 21/1000\n",
      "----------\n",
      "Train loss 0.02410510496664439 accuracy 0.9939951210358416\n",
      "Val   loss 1.726042098548002 accuracy 0.7828371278458844\n",
      "\n",
      "Epoch 22/1000\n",
      "----------\n",
      "Train loss 0.03128737289842095 accuracy 0.9921185963595421\n",
      "Val   loss 1.8287120421756906 accuracy 0.7854640980735552\n",
      "\n",
      "Epoch 23/1000\n",
      "----------\n",
      "Train loss 0.028262678076535347 accuracy 0.9936198161005818\n",
      "Val   loss 1.6425262670226555 accuracy 0.8117338003502627\n",
      "\n",
      "Epoch 24/1000\n",
      "----------\n",
      "Train loss 0.048035215277122095 accuracy 0.9898667667479828\n",
      "Val   loss 1.502622799080503 accuracy 0.7994746059544658\n",
      "\n",
      "Epoch 25/1000\n",
      "----------\n",
      "Train loss 0.027607963880021427 accuracy 0.9930568586976919\n",
      "Val   loss 1.480994117906448 accuracy 0.8056042031523643\n",
      "\n",
      "Epoch 26/1000\n",
      "----------\n",
      "Train loss 0.025921477028430347 accuracy 0.9932445111653219\n",
      "Val   loss 1.6666435969054116 accuracy 0.8134851138353766\n",
      "\n",
      "Epoch 27/1000\n",
      "----------\n",
      "Train loss 0.037118377880892436 accuracy 0.9921185963595421\n",
      "Val   loss 1.3722304314918194 accuracy 0.797723292469352\n",
      "\n",
      "Epoch 28/1000\n",
      "----------\n",
      "Train loss 0.028449852863624902 accuracy 0.9932445111653219\n",
      "Val   loss 1.447042098273717 accuracy 0.7959719789842382\n",
      "\n",
      "Epoch 29/1000\n",
      "----------\n",
      "Train loss 0.027980424901148437 accuracy 0.9938074685682117\n",
      "Val   loss 1.444346942559605 accuracy 0.8056042031523643\n",
      "\n",
      "Epoch 30/1000\n",
      "----------\n",
      "Train loss 0.034126325280366666 accuracy 0.9924939012948021\n",
      "Val   loss 1.4204486759338881 accuracy 0.7968476357267951\n",
      "\n",
      "Epoch 31/1000\n",
      "----------\n",
      "Train loss 0.029971547331538357 accuracy 0.9928692062300619\n",
      "Val   loss 1.6442325331384684 accuracy 0.7898423817863398\n",
      "\n",
      "Epoch 32/1000\n",
      "----------\n",
      "Train loss 0.026711730233225525 accuracy 0.9938074685682117\n",
      "Val   loss 1.5318314579290018 accuracy 0.8047285464098074\n",
      "\n",
      "Epoch 33/1000\n",
      "----------\n",
      "Train loss 0.025473431666450982 accuracy 0.9938074685682117\n",
      "Val   loss 1.5727433481197295 accuracy 0.8003502626970228\n",
      "\n",
      "Epoch 34/1000\n",
      "----------\n",
      "Train loss 0.024670476629480517 accuracy 0.9943704259711016\n",
      "Val   loss 1.550266966371737 accuracy 0.8143607705779334\n",
      "\n",
      "Epoch 35/1000\n",
      "----------\n",
      "Train loss 0.020789543982088994 accuracy 0.9956839932445112\n",
      "Val   loss 1.6089714015729442 accuracy 0.797723292469352\n",
      "\n",
      "Epoch 36/1000\n",
      "----------\n",
      "Train loss 0.02280465530372536 accuracy 0.9954963407768813\n",
      "Val   loss 1.7163035800837558 accuracy 0.7942206654991244\n",
      "\n",
      "Epoch 37/1000\n",
      "----------\n",
      "Train loss 0.016752903290950364 accuracy 0.9968099080502909\n",
      "Val   loss 1.6862625508623144 accuracy 0.809106830122592\n",
      "\n",
      "Epoch 38/1000\n",
      "----------\n",
      "Train loss 0.016951650632983546 accuracy 0.996434603115031\n",
      "Val   loss 1.4315973754087281 accuracy 0.809106830122592\n",
      "\n",
      "Epoch 39/1000\n",
      "----------\n",
      "Train loss 0.01515896421670129 accuracy 0.9968099080502909\n",
      "Val   loss 1.6620797931592126 accuracy 0.7994746059544658\n",
      "\n",
      "Epoch 40/1000\n",
      "----------\n",
      "Train loss 0.020312572437382024 accuracy 0.9958716457121412\n",
      "Val   loss 1.5999413741763722 accuracy 0.8126094570928196\n",
      "\n",
      "Epoch 41/1000\n",
      "----------\n",
      "Train loss 0.01748315944955874 accuracy 0.996434603115031\n",
      "Val   loss 1.8103853445283373 accuracy 0.7994746059544658\n",
      "\n",
      "Epoch 42/1000\n",
      "----------\n",
      "Train loss 0.024139592697938087 accuracy 0.9945580784387316\n",
      "Val   loss 1.6597103393033497 accuracy 0.8003502626970228\n",
      "\n",
      "Epoch 43/1000\n",
      "----------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-8630b6ab604b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     train_acc, train_loss = train_epoch(\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mtrain_data_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-38-aee46520f774>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, data_loader, loss_fn, optim, device, scheduler, n_examples)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = defaultdict(list)\n",
    "best_accuracy = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
    "    print('-' * 10)\n",
    "\n",
    "    train_acc, train_loss = train_epoch(\n",
    "        model,\n",
    "        train_data_loader,    \n",
    "        loss_fn, \n",
    "        optim, \n",
    "        device, \n",
    "        scheduler, \n",
    "        len(df_train)\n",
    "    )\n",
    "\n",
    "    print(f'Train loss {train_loss} accuracy {train_acc}')\n",
    "\n",
    "    val_acc, val_loss = eval_model(\n",
    "        model,\n",
    "        val_data_loader,\n",
    "        loss_fn, \n",
    "        device, \n",
    "        len(df_valid)\n",
    "    )\n",
    "\n",
    "    print(f'Val   loss {val_loss} accuracy {val_acc}')\n",
    "    print()\n",
    "\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "\n",
    "    if val_acc > best_accuracy:\n",
    "        torch.save(model.state_dict(), 'best_model_state.bin')\n",
    "        best_accuracy = val_acc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
