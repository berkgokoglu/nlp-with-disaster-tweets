{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup\n",
    "import nlpaug.augmenter.word as naw\n",
    "\n",
    "import torch\n",
    "from torch import tensor\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "from collections import OrderedDict\n",
    "import random\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "p = {\n",
    "    'BATCH_SZ': 10,\n",
    "    'MAX_LEN': 120,\n",
    "    'MODEL': 'bert-base-cased', #xlnet-base-cased xlm-mlm-en-2048\n",
    "    'TRAIN_VALID_SPLIT': 0.07,\n",
    "    'DROPOUT_0': 0.8,\n",
    "    'DROPOUT_1': 0.8,\n",
    "    'N_CLASSES': 2,\n",
    "    'CLIPPING': True,\n",
    "    'SCHEDULER': True,\n",
    "    'LR': 2e-5,\n",
    "    'LIN_0_HIDDEN_SZ': 256,\n",
    "    'ADDED_AUGMENTED_TWEETS': 4000,\n",
    "    'AUGMENTATION_MODEL': 'bert-base-cased'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "DATA = Path('/home/sharif/Documents/Challenges/nlp-with-disaster-tweets/data')\n",
    "train_df, test_df = pd.read_csv(DATA/'train.csv'), pd.read_csv(DATA/'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class DS(Dataset):\n",
    "    def __init__(self, texts, targets, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.targets = targets\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self): return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        text = str(self.texts[item])\n",
    "        target = self.targets[item]\n",
    "        \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "          text,\n",
    "          add_special_tokens=True,\n",
    "          max_length=self.max_len,\n",
    "          return_token_type_ids=False,\n",
    "          pad_to_max_length=True,\n",
    "          return_attention_mask=True,\n",
    "          return_tensors='pt',\n",
    "          truncation=True\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "          'text': text,\n",
    "          'input_ids': encoding['input_ids'].flatten(),\n",
    "          'attention_mask': encoding['attention_mask'].flatten(),\n",
    "          'targets': torch.tensor(target, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_df, valid_df = train_test_split(train_df, test_size=p['TRAIN_VALID_SPLIT'], random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_df.reset_index(inplace=True)\n",
    "valid_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7080, 533)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df), len(valid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>965</td>\n",
       "      <td>1395</td>\n",
       "      <td>body%20bag</td>\n",
       "      <td>Greenville,SC</td>\n",
       "      <td>@TR_jdavis Bruh you wanna fight I'm down meet ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7231</td>\n",
       "      <td>10355</td>\n",
       "      <td>weapons</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@NRO Except when ordered not to carry unauthor...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4421</td>\n",
       "      <td>6292</td>\n",
       "      <td>hostage</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Egyptian Militants Tied to ISIS Threaten to Ki...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7133</td>\n",
       "      <td>10217</td>\n",
       "      <td>volcano</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eruption of Indonesian volcano sparks transpor...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6276</td>\n",
       "      <td>8967</td>\n",
       "      <td>storm</td>\n",
       "      <td>Wilmington, NC</td>\n",
       "      <td>New item: Pillow Covers ANY SIZE Pillow Cover ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index     id     keyword        location  \\\n",
       "0    965   1395  body%20bag   Greenville,SC   \n",
       "1   7231  10355     weapons             NaN   \n",
       "2   4421   6292     hostage             NaN   \n",
       "3   7133  10217     volcano             NaN   \n",
       "4   6276   8967       storm  Wilmington, NC   \n",
       "\n",
       "                                                text  target  \n",
       "0  @TR_jdavis Bruh you wanna fight I'm down meet ...       0  \n",
       "1  @NRO Except when ordered not to carry unauthor...       1  \n",
       "2  Egyptian Militants Tied to ISIS Threaten to Ki...       1  \n",
       "3  Eruption of Indonesian volcano sparks transpor...       1  \n",
       "4  New item: Pillow Covers ANY SIZE Pillow Cover ...       0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Augmenatation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sharif/.local/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "aug = naw.ContextualWordEmbsAug(model_path=p['AUGMENTATION_MODEL'], action=\"substitute\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def augment(train_df):\n",
    "    n_tweets = p['ADDED_AUGMENTED_TWEETS']\n",
    "    for i in range(n_tweets):\n",
    "        if i % 100 == 0: print(f'{i}/{n_tweets}')\n",
    "        idx = random.randint(0, len(train_df))\n",
    "        target = train_df.target[idx]\n",
    "        text = train_df.text[idx]\n",
    "        text_aug = aug.augment(text)\n",
    "        train_df = train_df.append({'text': text_aug, 'target': target}, ignore_index=True)\n",
    "    return train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/4000\n",
      "100/4000\n",
      "200/4000\n",
      "300/4000\n",
      "400/4000\n",
      "500/4000\n",
      "600/4000\n",
      "700/4000\n",
      "800/4000\n",
      "900/4000\n",
      "1000/4000\n",
      "1100/4000\n",
      "1200/4000\n",
      "1300/4000\n",
      "1400/4000\n",
      "1500/4000\n",
      "1600/4000\n",
      "1700/4000\n",
      "1800/4000\n",
      "1900/4000\n",
      "2000/4000\n",
      "2100/4000\n",
      "2200/4000\n",
      "2300/4000\n",
      "2400/4000\n",
      "2500/4000\n",
      "2600/4000\n",
      "2700/4000\n",
      "2800/4000\n",
      "2900/4000\n",
      "3000/4000\n",
      "3100/4000\n",
      "3200/4000\n",
      "3300/4000\n",
      "3400/4000\n",
      "3500/4000\n",
      "3600/4000\n",
      "3700/4000\n",
      "3800/4000\n",
      "3900/4000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11080"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = augment(train_df); len(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
    "    ds = DS(\n",
    "        texts=df.text.to_numpy(),\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=max_len,\n",
    "        targets=df.target.to_numpy()\n",
    "    )\n",
    "    \n",
    "    return DataLoader(\n",
    "        ds,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=4,\n",
    "        shuffle=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Model + Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(p['MODEL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class BertClassifier(pl.LightningModule):\n",
    "    def __init__(self, train_df, valid_df, n_c=2, params=p):\n",
    "        super().__init__()\n",
    "        self.hparams = p\n",
    "        self.train_df, self.valid_df = train_df, valid_df\n",
    "        \n",
    "        self.bert = BertModel.from_pretrained(p['MODEL'])\n",
    "        self.drop0 = nn.Dropout(p=p['DROPOUT_0'])\n",
    "        self.drop1 = nn.Dropout(p=p['DROPOUT_1'])\n",
    "        self.lin0 = nn.Linear(self.bert.config.hidden_size, p['LIN_0_HIDDEN_SZ'])\n",
    "        self.lin1 = nn.Linear(p['LIN_0_HIDDEN_SZ'], p['N_CLASSES'])\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        _, pooled_output = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        y = self.drop0(pooled_output)\n",
    "        y = self.lin0(y)\n",
    "        y = self.drop1(y)\n",
    "        return self.lin1(y)\n",
    "    \n",
    "    def step(self, batch):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        targets = batch[\"targets\"]\n",
    "        \n",
    "        outputs = self(\n",
    "          input_ids=input_ids,\n",
    "          attention_mask=attention_mask\n",
    "        )\n",
    "        \n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "        \n",
    "        acc = (preds == targets).float().mean()\n",
    "        loss =  F.cross_entropy(outputs, targets)\n",
    "        return OrderedDict({\n",
    "            'loss': loss,\n",
    "            'accuracy': acc\n",
    "        })\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self.step(batch)\n",
    "    \n",
    "    def training_epoch_end(self, outputs):\n",
    "        loss_mean = torch.stack([output['loss'] for output in outputs]).float().mean()\n",
    "        acc_mean = torch.stack([output['accuracy'] for output in outputs]).float().mean()\n",
    "        self.log('train_loss', loss_mean)\n",
    "        self.log('train_accuracy', acc_mean, prog_bar=True)\n",
    "        \n",
    "        if p['CLIPPING']: nn.utils.clip_grad_norm_(self.parameters(), max_norm=1.0)\n",
    "        if p['SCHEDULER']: self.scheduler.step()\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self.step(batch)\n",
    "    \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        loss_mean = torch.stack([output['loss'] for output in outputs]).float().mean()\n",
    "        acc_mean = torch.stack([output['accuracy'] for output in outputs]).float().mean()\n",
    "        self.log('valid_loss', loss_mean, prog_bar=True)\n",
    "        self.log('valid_accuracy', acc_mean, prog_bar=True)\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        optim = AdamW(self.parameters(), lr=p['LR'], correct_bias=False) \n",
    "        self.scheduler = get_linear_schedule_with_warmup(\n",
    "          optim,\n",
    "          num_warmup_steps=0,\n",
    "          num_training_steps=len(self.train_dataloader())*100\n",
    "        )\n",
    "        return optim\n",
    "            \n",
    "    def train_dataloader(self): return create_data_loader(self.train_df, tokenizer, p['MAX_LEN'], p['BATCH_SZ'])\n",
    "    def val_dataloader(self): return create_data_loader(self.valid_df, tokenizer, p['MAX_LEN'], p['BATCH_SZ'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "classifier = BertClassifier(train_df, valid_df, params=p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(\n",
    "    monitor='valid_accuracy',\n",
    "    filename='transformer-{epoch:02d}-{valid_accuracy:.4f}',\n",
    "    save_top_k=3,\n",
    "    mode='max'\n",
    ")\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='valid_accuracy',\n",
    "    patience=30,\n",
    "    verbose=True,\n",
    "    mode='max'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type      | Params\n",
      "------------------------------------\n",
      "0 | bert  | BertModel | 108 M \n",
      "1 | drop0 | Dropout   | 0     \n",
      "2 | drop1 | Dropout   | 0     \n",
      "3 | lin0  | Linear    | 196 K \n",
      "4 | lin1  | Linear    | 514   \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validation sanity check'), FloatProgress(value=1.0, bar_style='info', layout=Layout…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c88bcf544d54d289b444056f53c1ccc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Training'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = pl.Trainer(gpus=1, max_epochs=100, deterministic=True, callbacks=[checkpoint, early_stopping])#, fast_dev_run=True)\n",
    "trainer.fit(classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ckp_f = '/home/sharif/Documents/Challenges/nlp-with-disaster-tweets/lightning_logs/architecture_1/version_0/checkpoints/transformer-epoch=10-valid_accuracy=0.8136.ckpt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "classifier = BertClassifier(None, None, params=p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def load_model(p):\n",
    "    ckp = torch.load(p)\n",
    "    classifier.load_state_dict(ckp['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "load_model(ckp_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(p['MODEL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def predict(text):\n",
    "    encoding = tokenizer.encode_plus(\n",
    "              text,\n",
    "              add_special_tokens=True,\n",
    "              max_length=120,\n",
    "              return_token_type_ids=False,\n",
    "              pad_to_max_length=True,\n",
    "              return_attention_mask=True,\n",
    "              return_tensors='pt',\n",
    "              truncation=True\n",
    "    )\n",
    "    y_hat = classifier(encoding['input_ids'], encoding['attention_mask'])\n",
    "    return y_hat.argmax().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def accuracy():\n",
    "    correct = 0.0\n",
    "    for i, row in enumerate(valid_df.values):\n",
    "        if i % 100 == 0: \n",
    "            print(f'{i}/{len(valid_df)}')\n",
    "            print(correct/(i+1))\n",
    "        s = row[3]\n",
    "        y = int(row[4])\n",
    "        y_hat = predict(s)\n",
    "        correct += int(y_hat == y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/533\n",
      "0.0\n",
      "100/533\n",
      "0.8217821782178217\n",
      "200/533\n",
      "0.8009950248756219\n",
      "300/533\n",
      "0.8006644518272426\n",
      "400/533\n",
      "0.8179551122194514\n",
      "500/533\n",
      "0.8183632734530938\n"
     ]
    }
   ],
   "source": [
    "accuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/3263\n",
      "100/3263\n",
      "200/3263\n",
      "300/3263\n",
      "400/3263\n",
      "500/3263\n",
      "600/3263\n",
      "700/3263\n",
      "800/3263\n",
      "900/3263\n",
      "1000/3263\n",
      "1100/3263\n",
      "1200/3263\n",
      "1300/3263\n",
      "1400/3263\n",
      "1500/3263\n",
      "1600/3263\n",
      "1700/3263\n",
      "1800/3263\n",
      "1900/3263\n",
      "2000/3263\n",
      "2100/3263\n",
      "2200/3263\n",
      "2300/3263\n",
      "2400/3263\n",
      "2500/3263\n",
      "2600/3263\n",
      "2700/3263\n",
      "2800/3263\n",
      "2900/3263\n",
      "3000/3263\n",
      "3100/3263\n",
      "3200/3263\n"
     ]
    }
   ],
   "source": [
    "preds = []\n",
    "for i, row in enumerate(test_df.values):\n",
    "    if i % 100 == 0: print(f'{i}/{len(test_df)}')\n",
    "    id = row[0]\n",
    "    s = row[-1]\n",
    "    y_hat = predict(s)\n",
    "    preds.append([id, y_hat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    target\n",
       "id        \n",
       "0        1\n",
       "2        1\n",
       "3        1\n",
       "9        1\n",
       "11       1"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.DataFrame(preds, columns=['id', 'target']).set_index('id')\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "submission.to_csv('submission.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
